{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from datasets import list_datasets, load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_excel_path = '/home/philko/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data/raw/Lemmaliste.xlsx'\n",
    "corpus_data_json_path = '/home/philko/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data.json'\n",
    "corpus = pd.read_excel(corpus_excel_path)['Lemmata'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43472"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43462"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus.dropna()\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmata</th>\n",
       "      <th>list</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acarus</td>\n",
       "      <td>[a, c, a, r, u, s]</td>\n",
       "      <td>a c a r u s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>[a]</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aano</td>\n",
       "      <td>[a, a, n, o]</td>\n",
       "      <td>a a n o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aardum</td>\n",
       "      <td>[a, a, r, d, u, m]</td>\n",
       "      <td>a a r d u m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aardus</td>\n",
       "      <td>[a, a, r, d, u, s]</td>\n",
       "      <td>a a r d u s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Lemmata                list         text\n",
       "0  acarus  [a, c, a, r, u, s]  a c a r u s\n",
       "1       a                 [a]            a\n",
       "2    aano        [a, a, n, o]      a a n o\n",
       "3  aardum  [a, a, r, d, u, m]  a a r d u m\n",
       "4  aardus  [a, a, r, d, u, s]  a a r d u s"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_excel_path = '/home/philko/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data/raw/Lemmaliste.xlsx'\n",
    "corpus_data_json_path = '/home/philko/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data.json'\n",
    "corpus = pd.read_excel(corpus_excel_path)['Lemmata'].to_frame()\n",
    "# corpus = corpus.dropna()\n",
    "corpus_data_json = pd.read_json(corpus_data_json_path)\n",
    "# corpus_data_json = corpus_data_json.dropna()\n",
    "corpus_data_json = corpus_data_json.rename(columns={'lemma': 'Lemmata'})\n",
    "corpus['Lemmata'] = corpus['Lemmata'].astype(str)\n",
    "corpus_data_json['Lemmata'] = corpus_data_json['Lemmata'].astype(str)\n",
    "corpus_data_json = corpus_data_json.drop(\"id\", axis=1)\n",
    "corpus_data_json = corpus_data_json.drop_duplicates()\n",
    "# corpus.join(corpus_data_json, on = \"Lemmata\", how = \"inner\")\n",
    "corpus = pd.concat([corpus, corpus_data_json])\n",
    "\n",
    "rm_list: list = []\n",
    "for i, e in enumerate(corpus['Lemmata'].values):\n",
    "    if isinstance(e, float):\n",
    "        rm_list.append(e)\n",
    "corpus = corpus[~corpus['Lemmata'].isin(rm_list)]\n",
    "corpus['list'] = list(map(lambda e: list(e), corpus['Lemmata'].values))\n",
    "corpus['text'] = list(map(lambda e: \" \".join(e), corpus['list'].values))\n",
    "corpus.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmata</th>\n",
       "      <th>list</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acarus</td>\n",
       "      <td>[a, c, a, r, u, s]</td>\n",
       "      <td>a c a r u s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>[a]</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aano</td>\n",
       "      <td>[a, a, n, o]</td>\n",
       "      <td>a a n o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aardum</td>\n",
       "      <td>[a, a, r, d, u, m]</td>\n",
       "      <td>a a r d u m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aardus</td>\n",
       "      <td>[a, a, r, d, u, s]</td>\n",
       "      <td>a a r d u s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114419</th>\n",
       "      <td>nimbus</td>\n",
       "      <td>[n, i, m, b, u, s]</td>\n",
       "      <td>n i m b u s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114423</th>\n",
       "      <td>nimietas</td>\n",
       "      <td>[n, i, m, i, e, t, a, s]</td>\n",
       "      <td>n i m i e t a s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114431</th>\n",
       "      <td>nimirum</td>\n",
       "      <td>[n, i, m, i, r, u, m]</td>\n",
       "      <td>n i m i r u m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114458</th>\n",
       "      <td>nimis</td>\n",
       "      <td>[n, i, m, i, s]</td>\n",
       "      <td>n i m i s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114618</th>\n",
       "      <td>nimium</td>\n",
       "      <td>[n, i, m, i, u, m]</td>\n",
       "      <td>n i m i u m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46979 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lemmata                      list             text\n",
       "0         acarus        [a, c, a, r, u, s]      a c a r u s\n",
       "1              a                       [a]                a\n",
       "2           aano              [a, a, n, o]          a a n o\n",
       "3         aardum        [a, a, r, d, u, m]      a a r d u m\n",
       "4         aardus        [a, a, r, d, u, s]      a a r d u s\n",
       "...          ...                       ...              ...\n",
       "114419    nimbus        [n, i, m, b, u, s]      n i m b u s\n",
       "114423  nimietas  [n, i, m, i, e, t, a, s]  n i m i e t a s\n",
       "114431   nimirum     [n, i, m, i, r, u, m]    n i m i r u m\n",
       "114458     nimis           [n, i, m, i, s]        n i m i s\n",
       "114618    nimium        [n, i, m, i, u, m]      n i m i u m\n",
       "\n",
       "[46979 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acarus', 'a', 'aano', ..., 'nimirum', 'nimis', 'nimium'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['Lemmata'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"test.txt\", corpus['Lemmata'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acarus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aardum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aardus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46973</th>\n",
       "      <td>nimbus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46974</th>\n",
       "      <td>nimietas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46975</th>\n",
       "      <td>nimirum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46976</th>\n",
       "      <td>nimis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46977</th>\n",
       "      <td>nimium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46978 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         acarus\n",
       "0             a\n",
       "1          aano\n",
       "2        aardum\n",
       "3        aardus\n",
       "4           aoo\n",
       "...         ...\n",
       "46973    nimbus\n",
       "46974  nimietas\n",
       "46975   nimirum\n",
       "46976     nimis\n",
       "46977    nimium\n",
       "\n",
       "[46978 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Lemmata', 'list', 'text', '__index_level_0__'],\n",
       "    num_rows: 46979\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(corpus)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000 # Check effect of modifying this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), BATCH_SIZE):\n",
    "        yield dataset[i : i + BATCH_SIZE][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "special_tokens_map = {\n",
    "    'cls_token': '<CLS>',\n",
    "    'pad_token':'<PAD>',\n",
    "    'sep_token': '<SEP>',\n",
    "    'bos_token': '<|begoftext|>',\n",
    "    'eos_token': '<|endoftext|>',\n",
    "    'unk_token': '<UNK>'}\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(list(special_tokens_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '#',\n",
       " '*',\n",
       " '-',\n",
       " '.',\n",
       " '>',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'E',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Z',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet: set = set(functools.reduce(lambda x, y: x + y, corpus['list'], []))\n",
    "alphabet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(\n",
    "        lemma_list_path: str,\n",
    "        tokenizer_path: str,\n",
    "        batch_size: int = 1000) -> Tokenizer:\n",
    "    \"\"\"Train Tokenizer.\n",
    "    \n",
    "    Load corpus, split words to characters and train tokenizer on them.\n",
    "    Tokenizer will also be saved at a specified ('tokenizer path')\n",
    "    location.\n",
    "    \n",
    "    :param lemma_list_path: Path to lemma .xlsx file.\n",
    "    :paramm tokenizer_path: Path where tokenizer is to be saved.\n",
    "    :param batch_size: Batch size for training.\n",
    "    :returns: Trained tokenizer\n",
    "    \"\"\"\n",
    "    corpus: pd.DataFrame = pd.read_excel(lemma_list_path)['Lemmata'].to_frame()\n",
    "    rm_list: list = []\n",
    "    for i, e in enumerate(corpus['Lemmata'].values):\n",
    "        if isinstance(e, float):\n",
    "            rm_list.append(e)\n",
    "    corpus = corpus[~corpus['Lemmata'].isin(rm_list)]\n",
    "    corpus['list'] = list(map(lambda e: list(e), corpus['Lemmata'].values))\n",
    "    corpus['text'] = list(map(lambda e: \" \".join(e), corpus['list'].values))\n",
    "\n",
    "    dataset = Dataset.from_pandas(corpus)\n",
    "\n",
    "    alphabet: set = set(functools.reduce(lambda x, y: x + y, corpus['list'], []))\n",
    "\n",
    "    tokenizer: Tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "    special_tokens_map = {\n",
    "        'cls_token': '<CLS>',\n",
    "        'pad_token':'<PAD>',\n",
    "        'sep_token': '<SEP>',\n",
    "        'bos_token': '<|begoftext|>',\n",
    "        'eos_token': '<|endoftext|>',\n",
    "        'unk_token': '<UNK>'}\n",
    "\n",
    "    num_added_toks = tokenizer.add_special_tokens(list(special_tokens_map))\n",
    "\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<|begoftext|> $A <|endoftext|>\",\n",
    "        special_tokens=[(\"<|begoftext|>\", 1), (\"<|endoftext|>\", 2)],\n",
    "    )\n",
    "\n",
    "    def batch_iterator():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            yield dataset[i : i + batch_size][\"text\"]\n",
    "\n",
    "    # Train tokenizer\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=len(alphabet),\n",
    "        special_tokens=list(special_tokens_map)) # Check recommendations for vocabulary size\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        batch_iterator(),\n",
    "        trainer=trainer)\n",
    "\n",
    "    # Post-processor and decoder\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False, )\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = train_tokenizer(\n",
    "    '/home/philko/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data/raw/Lemmaliste.xlsx',\n",
    "    \"2-byte-level-BPE.tokenizer.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tokenizers.Tokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39;49m\u001b[39mgardo\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tokenizers.Tokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "a = tokenizer(\"gardo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Check if special chars are necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train tokenizer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=len(alphabet),\n",
    "    special_tokens=list(special_tokens_map)) # Check recommendations for vocabulary size\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    batch_iterator(),\n",
    "    trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x8910dd0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 24, 35, 24, 37, 28]\n",
      "cls_tokenpad_tokensep_tokenbos_tokeneos_tokenunk_token\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Post-processor and decoder\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False, )\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Sanity check\n",
    "\n",
    "print(tokenizer.encode(\"kalane\").ids)\n",
    "print(tokenizer.decode([0,1,2,3,4,5,423], skip_special_tokens = False))\n",
    "\n",
    "# Save the tokenizer you trained\n",
    "tokenizer.save(\"byte-level-BPE.tokenizer.json\")\n",
    "\n",
    "# Load it using transformers (required, otherwise it is not a callable object)\n",
    "# tokenizer = PreTrainedTokenizerFast(tokenizer_file= path + \"byte-level-BPE.tokenizer.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it using transformers (required, otherwise it is not a callable object)\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"2-byte-level-BPE.tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer(\"g a r d o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [30, 51, 24, 51, 41, 51, 27, 51, 38], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g a r d o'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(a['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer(\"gardo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus: pd.DataFrame = pd.read_excel('/home/philko/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data/raw/Lemmaliste.xlsx')['Lemmata'].to_frame()\n",
    "rm_list: list = []\n",
    "for i, e in enumerate(corpus['Lemmata'].values):\n",
    "    if isinstance(e, float):\n",
    "        rm_list.append(e)\n",
    "corpus = corpus[~corpus['Lemmata'].isin(rm_list)]\n",
    "corpus['list'] = list(map(lambda e: list(e), corpus['Lemmata'].values))\n",
    "corpus['text'] = list(map(lambda e: \" \".join(e), corpus['list'].values))\n",
    "\n",
    "dataset = Dataset.from_pandas(corpus)\n",
    "\n",
    "alphabet: set = set(functools.reduce(lambda x, y: x + y, corpus['list'], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lemmata': 'acarus',\n",
       " 'list': ['a', 'c', 'a', 'r', 'u', 's'],\n",
       " 'text': 'a c a r u s',\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[1, 0, 577, 0, 54, 0, 114, 41, 0, 0, 44, 52, 1944, 1970, 50, 0, 0, 2]\n",
      "ello y al l o w are yo u\n"
     ]
    }
   ],
   "source": [
    "# I use tutorial code from https://huggingface.co/docs/tokenizers/quicktour as example\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[BOS]\", \"[EOS]\"]) #  Adding [BOS] and [EOS] here\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Please using TemplateProcessing\n",
    "# https://huggingface.co/docs/tokenizers/api/post-processors#tokenizers.processors.TemplateProcessing\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    special_tokens=[(\"[BOS]\", 1), (\"[EOS]\", 2)],\n",
    ")\n",
    "##################################################\n",
    "\n",
    "files = [\"vocab.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
    "print(output.ids)\n",
    "# >> [1, 27255, 18, 95, 13, 5099, 7, 7963, 5114, 6220, 0, 37, 2] <-- you can see there are token [1] in the begining and token [2] at the end of the sequence\n",
    "print(tokenizer.decode(output.ids))\n",
    "# >> no [BOS] and [EOS] after decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[BOS]\", \"[EOS]\"]) #  Adding [BOS] and [EOS] here\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    special_tokens=[(\"[BOS]\", 1), (\"[EOS]\", 2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50)\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), 128):\n",
    "        yield dataset[i : i + 128][\"Lemmata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at 'Missing additional token', /__w/tokenizers/tokenizers/tokenizers/src/tokenizer/added_vocabulary.rs:292:21\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "Missing additional token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39;49mtrain_from_iterator(\n\u001b[1;32m      2\u001b[0m     batch_iterator(),\n\u001b[1;32m      3\u001b[0m     trainer\u001b[39m=\u001b[39;49mtrainer)\n",
      "\u001b[0;31mPanicException\u001b[0m: Missing additional token"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    batch_iterator(),\n",
    "    trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"gardov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g a r d o v'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
