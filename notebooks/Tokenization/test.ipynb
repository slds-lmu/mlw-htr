{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from datasets import list_datasets, load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acarus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aardum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aardus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Lemmata\n",
       "0  acarus\n",
       "1       a\n",
       "2    aano\n",
       "3  aardum\n",
       "4  aardus"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_excel_path = '/home/USER/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data/raw/Lemmaliste.xlsx'\n",
    "corpus_data_json_path = '/home/USER/Documents/Uni/WiSe2223/Consulting/mlw-consulting-project/data.json'\n",
    "corpus = pd.read_excel(corpus_excel_path)['Lemmata'].to_frame()\n",
    "# corpus = corpus.dropna()\n",
    "corpus_data_json = pd.read_json(corpus_data_json_path)\n",
    "# corpus_data_json = corpus_data_json.dropna()\n",
    "corpus_data_json = corpus_data_json.rename(columns={'lemma': 'Lemmata'})\n",
    "corpus['Lemmata'] = corpus['Lemmata'].astype(str)\n",
    "corpus_data_json['Lemmata'] = corpus_data_json['Lemmata'].astype(str)\n",
    "corpus_data_json = corpus_data_json.drop(\"id\", axis=1)\n",
    "corpus_data_json = corpus_data_json.drop_duplicates()\n",
    "# corpus.join(corpus_data_json, on = \"Lemmata\", how = \"inner\")\n",
    "corpus = pd.concat([corpus, corpus_data_json])\n",
    "\n",
    "rm_list: list = []\n",
    "for i, e in enumerate(corpus['Lemmata'].values):\n",
    "    if isinstance(e, float):\n",
    "        rm_list.append(e)\n",
    "corpus = corpus[~corpus['Lemmata'].isin(rm_list)]\n",
    "corpus.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Lemmata', '__index_level_0__'],\n",
       "    num_rows: 46979\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(corpus)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[0, 32, 26, 43, 29, 40, 2]\n",
      "[BOS] g a r d o [EOS]\n"
     ]
    }
   ],
   "source": [
    "# I use tutorial code from https://huggingface.co/docs/tokenizers/quicktour as example\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(vocab_size=50, special_tokens=[\"[BOS]\", \"[UNK]\", \"[EOS]\"]) #  Adding [BOS] and [EOS] here\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Please using TemplateProcessing\n",
    "# https://huggingface.co/docs/tokenizers/api/post-processors#tokenizers.processors.TemplateProcessing\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[BOS] $A [EOS]\",\n",
    "        special_tokens=[(\"[BOS]\", 0 ), (\"[EOS]\", 2)],\n",
    ")\n",
    "\n",
    "##################################################\n",
    "\n",
    "files = [\"vocab.txt\"]\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), 128):\n",
    "        yield dataset[i : i + 128][\"Lemmata\"]\n",
    "tokenizer.train_from_iterator(\n",
    "        batch_iterator(),\n",
    "        trainer=trainer)\n",
    "\n",
    "output = tokenizer.encode(\"gardo\")\n",
    "print(output.ids)\n",
    "# >> [1, 27255, 18, 95, 13, 5099, 7, 7963, 5114, 6220, 0, 37, 2] <-- you can see there are token [1] in the begining and token [2] at the end of the sequence\n",
    "print(tokenizer.decode(output.ids, skip_special_tokens=False))\n",
    "# >> no [BOS] and [EOS] after decoding\n",
    "\n",
    "tokenizer.save(\"byte-level-BPE.tokenizerTEST.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens([\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_map = {'cls_token': '<CLS>', 'pad_token':'<PAD>', 'sep_token': '<SEP>', 'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<UNK>', 'additional_special_tokens': '@'}\n",
    "num_added_toks = tokenizer.add_special_tokens(list(special_tokens_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': 'dict' object cannot be converted to 'PyList'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m special_tokens_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcls_token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m<CLS>\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbos_token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m[BOS]\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39meos_token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m<|endoftext|>\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39munk_token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m----> 7\u001b[0m tokenizer\u001b[39m.\u001b[39;49madd_special_tokens(special_tokens_dict)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'tokens': 'dict' object cannot be converted to 'PyList'"
     ]
    }
   ],
   "source": [
    "special_tokens_dict = {\n",
    "    'pad_token': '<PAD>',\n",
    "    'cls_token': '<CLS>',\n",
    "    'bos_token': '[BOS]',\n",
    "    'eos_token': '<|endoftext|>',\n",
    "    'unk_token': '<UNK>'}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
